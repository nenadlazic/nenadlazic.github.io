<!DOCTYPE html>
<html lang="en-us" class="m-auto  dark "><head>
  <title>Nenad Laziƒá</title>

<meta name="theme-color" content="" />
<meta charset="utf-8" />
<meta content="width=device-width, initial-scale=1.0" name="viewport" />
<meta name="description" content="Software Engineer ‚Äî Portfolio, Blog &amp; Contact" />
<meta name="author" content="Nenad Laziƒá" />
<meta name="generator" content="aafu theme by Darshan in Hugo 0.147.3" />

        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">        <link rel="manifest" href="/site.webmanifest">        <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#252627">        <link rel="shortcut icon" href="/favicon.ico">
  <link
    rel="stylesheet"
    href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu"
    crossorigin="anonymous"
  />
  <link
    rel="stylesheet"
    href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"
  />
  <link
    rel="stylesheet"
    href="//fonts.googleapis.com/css?family=Didact+Gothic%7CRoboto:400%7CRoboto+Mono"
  />

  
  
  
  <link
    rel="stylesheet"
    href="/main.min.ac639589c935e0fbc3b65978877052f518f4095b63502dfa8492adb2ad508338.css"
    integrity="sha256-rGOVick14PvDtll4h3BS9Rj0CVtjUC36hJKtsq1Qgzg="
    crossorigin="anonymous"
  />
  

  <link href="/main.css" rel="stylesheet" />
  <link rel="stylesheet" href="/css/general.css" />
  <link rel="stylesheet" href="/css/search.css" />

  
    <meta property="og:image" content="https://nenadlazic.github.io/images/default-preview.png">
  

  <script>
    let html = document.querySelector("html");
    let theme = window.localStorage.getItem("theme");

    const setTheme = (theme) => {
      html.classList.remove("light");
      if (theme === "dark") {
        html.classList.add("dark");
        window.localStorage.setItem("theme", "dark");
      } else {
        html.classList.remove("dark");
        window.localStorage.setItem("theme", "light");
      }
      fixThemeToggleIcon(theme);
    };

    const fixThemeToggleIcon = (theme) => {
      let themeToggle = document.querySelector(".theme-toggle");
      if (themeToggle) {
        if (theme === "dark") {
          themeToggle.classList.remove("fa-moon");
          themeToggle.classList.add("fa-sun");
        } else {
          themeToggle.classList.remove("fa-sun");
          themeToggle.classList.add("fa-moon");
        }
      }
    };

    if (theme == null) {
      if (html.classList.contains("dark")) {
        theme = "dark";
      } else if (html.classList.contains("light")) {
        theme = "light";
      } else {
        
        const prefersDark = window.matchMedia(
          "(prefers-color-scheme: dark)"
        ).matches;
        if (prefersDark) {
          theme = "dark";
        } else {
          theme = "light";
        }
      }
    }

    setTheme(theme);

    const toggleTheme = () => {
      html.classList.contains("dark") ? setTheme("light") : setTheme("dark");
    };

    window.onload = () => {
      fixThemeToggleIcon(theme);

      let defaultActivePanel = document.querySelector(".accordion.active");
      if (defaultActivePanel) {
        defaultActivePanel.nextElementSibling.style.maxHeight =
          defaultActivePanel.nextElementSibling.scrollHeight + "px";
      }
    };

    window.onresize = () => {
      let defaultActivePanel = document.querySelector(".accordion.active");
      if (defaultActivePanel) {
        defaultActivePanel.nextElementSibling.style.maxHeight =
          defaultActivePanel.nextElementSibling.scrollHeight + "px";
      }
    };
  </script>
</head>
<body class="h-screen p-2 m-auto max-w-4xl flex flex-col">
    
    <header
  class="nav flex flex-row row py-2 mb-6 w-full border-b border-gray-700 dark:border-gray-300 justify-between"
>
  <div>
    <a class="nav-menu-item" href="https://nenadlazic.github.io/">Home</a>
    <a class="nav-menu-item" href="/blog">Blog</a>
  </div>
  <div>
    <a class="mr-4" href="/search">
      <i class="fas fa-search"></i>
    </a>
    <i
      class="fas fa-sun theme-toggle text-blue-500 hover:text-blue-700 dark:text-yellow-300 dark:hover:text-yellow-500 cursor-pointer text-lg mr-9 sm:mr-0"
      onclick="toggleTheme()"
    ></i>
  </div>
</header>



    
    <main class="grow">
<div class="prose prose-stone dark:prose-invert max-w-none">
<div class="mb-3">
  <h1 class="top-h1">Fine-Tuning Tesseract OCR Models with Docker: A Practical Guide</h1>
  <p class="mb-1">February 13, 2026</p>
  <p>&mdash;</p>
</div>
<div class="content">
  <p>Training custom Tesseract OCR models can significantly improve text recognition accuracy for specific use cases - whether you&rsquo;re dealing with unique fonts, specialized documents, or specific languages. This practical guide walks you through the entire process using a Docker-based workflow that eliminates environment setup headaches.</p>
<p>In this guide, we&rsquo;ll cover:</p>
<ul>
<li>Why fine-tuning Tesseract matters</li>
<li>Setting up the Docker training environment</li>
<li>Preparing your training data correctly</li>
<li>Running the training process</li>
<li>Evaluating model performance with CER/WER metrics</li>
<li>Tips for achieving better results</li>
</ul>
<h2 id="why-fine-tune-tesseract">Why Fine-Tune Tesseract?</h2>
<p>Default Tesseract models work well for major languages, but quickly show limits in real-world use.</p>
<p>Fine-tuning is essential when working with:</p>
<ul>
<li><strong>Under-resourced languages</strong> - Minority and regional languages lack sufficient training data, making fine-tuning essential for acceptable accuracy</li>
<li><strong>Specialized fonts</strong> - Historical documents, stylized text, custom typefaces often go unrecognized</li>
<li><strong>Domain-specific content</strong> - Medical records, legal documents, technical drawings with specialized terminology</li>
<li><strong>Low-quality scans</strong> - Degraded historical documents, poor photocopies, faxes degrade recognition quality</li>
<li><strong>Unique layouts</strong> - Forms, tables, mixed-language documents confuse generic models</li>
</ul>
<p>For languages with minimal community support, fine-tuning transforms Tesseract from barely functional to production-ready.</p>
<p>Fine-tuning starts with a pre-trained model and adapts it to your specific data. You don&rsquo;t need thousands of samples - often 300-400 well-chosen examples yield significant improvements.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before starting, ensure you have:</p>
<ol>
<li><strong>Docker installed</strong> on your system</li>
<li><strong>Training data prepared</strong> - PNG images with corresponding ground truth text files</li>
<li><strong>Basic command line familiarity</strong></li>
</ol>
<h2 id="fine-tuning-steps">Fine-Tuning Steps</h2>
<p>To simplify the fine-tuning process, I created a Docker image that removes all environment setup complexity. Instead of manually managing Ubuntu versions, system libraries, Python packages, and Tesseract dependencies, everything runs inside an isolated container.</p>
<p>The workflow is straightforward and consists of six steps:</p>
<ol>
<li><strong>Prepare your data</strong> - Create image/text pairs in the correct format</li>
<li><strong>Configure training</strong> - Set model parameters in a simple config file</li>
<li><strong>Build the environment</strong> - One-time Docker image build</li>
<li><strong>Run training</strong> - Single command starts the entire pipeline</li>
<li><strong>Evaluate results</strong> - Automated comparison against baseline model</li>
<li><strong>Deploy your model</strong> - Use the <code>.traineddata</code> file in production</li>
</ol>
<p>The entire process is automated through shell scripts that handle all complexity inside the Docker container.</p>
<p>Let‚Äôs get started.</p>
<h2 id="step-1-prepare-your-training-data">Step 1: Prepare Your Training Data</h2>
<p>The quality of your training data directly impacts model accuracy. Here&rsquo;s how to prepare it properly:</p>
<h3 id="data-format">Data Format</h3>
<p>You need pairs of files:</p>
<ul>
<li><strong>Image file</strong>: PNG format (e.g., <code>sample_001.png</code>)</li>
<li><strong>Ground truth file</strong>: Text file with <code>.gt.txt</code> extension containing the exact text from the image</li>
</ul>
<p>Organize your prepared data in the WORKDIR as follows, splitting roughly 70% for training and 30% for evaluation:</p>
<pre tabindex="0"><code>WORKDIR/training_data/
‚îú‚îÄ‚îÄ sample_001.png
‚îú‚îÄ‚îÄ sample_001.gt.txt
‚îú‚îÄ‚îÄ sample_002.png
‚îú‚îÄ‚îÄ sample_002.gt.txt
‚îî‚îÄ‚îÄ ...
WORKDIR/evaluation_data/
‚îú‚îÄ‚îÄ eval_sample_001.png
‚îú‚îÄ‚îÄ eval_sample_001.gt.txt
‚îú‚îÄ‚îÄ eval_sample_002.png
‚îú‚îÄ‚îÄ eval_sample_002.gt.txt
‚îî‚îÄ‚îÄ ...
</code></pre><h3 id="important-rules">Important Rules</h3>
<p><strong>1. Exact matching</strong></p>
<p>Your <code>.gt.txt</code> file must contain the <strong>exact</strong> text visible in the image:</p>
<ul>
<li>Same capitalization</li>
<li>Same punctuation</li>
<li>Same spacing</li>
<li>Same line breaks</li>
</ul>
<p>If the image shows &ldquo;Dr. Smith&rdquo; but your <code>.gt.txt</code> says &ldquo;Dr Smith&rdquo;, the model learns incorrect patterns.</p>
<p><strong>2. File naming match</strong></p>
<p>Each PNG must have a corresponding <code>.gt.txt</code> with the same base name:</p>
<ul>
<li>‚úÖ <code>doc_001.png</code> + <code>doc_001.gt.txt</code></li>
<li>‚ùå <code>doc_001.png</code> + <code>document_001.gt.txt</code></li>
</ul>
<p><strong>3. PNG format only</strong></p>
<p>All images must be PNG format. Convert JPG and other formats before training:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>convert input.jpg output.png
</span></span></code></pre></div><p>PNG is required because Tesseract&rsquo;s training pipeline expects lossless image data.</p>
<h3 id="how-much-data">How Much Data?</h3>
<ul>
<li><strong>Minimum</strong>: 50-100 samples for simple improvements</li>
<li><strong>Recommended</strong>: 200-500 samples for good results</li>
<li><strong>Optimal</strong>: 1000+ samples for production models</li>
</ul>
<p>Quality beats quantity: 100 well-prepared samples outperform 1,000 rushed ones.</p>
<h3 id="data-diversity">Data Diversity</h3>
<p>Include variety in your training set:</p>
<ul>
<li>Different font sizes (if applicable)</li>
<li>Various text densities (sparse vs. dense)</li>
<li>Different quality levels (if you&rsquo;ll encounter both)</li>
<li>Edge cases (special characters, numbers, punctuation)</li>
</ul>
<h2 id="step-2-configure-your-training">Step 2: Configure Your Training</h2>
<p>Start by checking out your Git repository:</p>
<pre tabindex="0"><code>git clone https://github.com/nenadlazic/tesseract-training-docker

cd tesseract-training-docker
</code></pre><p>Then create or edit <code>config.env</code> in project root:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Model naming</span>
</span></span><span style="display:flex;"><span>MODEL_NAME<span style="color:#f92672">=</span>my_custom_model_v1
</span></span><span style="display:flex;"><span>START_MODEL<span style="color:#f92672">=</span>srp_latn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Training parameters</span>
</span></span><span style="display:flex;"><span>MAX_ITERATIONS<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>LEARNING_RATE<span style="color:#f92672">=</span>0.0001
</span></span><span style="display:flex;"><span>RATIO_TRAIN<span style="color:#f92672">=</span>0.9
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Paths</span>
</span></span><span style="display:flex;"><span>TESSDATA<span style="color:#f92672">=</span>/usr/share/tesseract-ocr/5/tessdata
</span></span><span style="display:flex;"><span>WORKDIR<span style="color:#f92672">=</span>WORKDIR
</span></span></code></pre></div><h3 id="parameter-explanations">Parameter Explanations</h3>
<p><strong>MODEL_NAME</strong>: Your output model identifier</p>
<ul>
<li>Use descriptive names: <code>legal_docs_v1</code>, <code>handwritten_forms_v2</code></li>
<li>Versioning helps track iterations</li>
</ul>
<p><strong>START_MODEL</strong>: Base model to fine-tune from</p>
<ul>
<li>Available: <code>srp_latn</code>, <code>hrv</code>, <code>slv</code>, <code>mkd</code>, <code>bul</code>, <code>ron</code>, <code>ell</code>, <code>srp</code> (easily expandable for other languages)</li>
<li>Choose the closest language/script to your target</li>
</ul>
<p><strong>MAX_ITERATIONS</strong>: How long to train</p>
<ul>
<li>Start with 100-500 for testing</li>
<li>Use 1000-5000 for production models</li>
<li>More iterations = longer training, potentially better accuracy</li>
<li>Watch for overfitting (model memorizes training data)</li>
</ul>
<p><strong>LEARNING_RATE</strong>: How aggressively to update weights</p>
<ul>
<li><code>0.0001</code> - Conservative, good for fine-tuning existing models</li>
<li><code>0.001</code> - Moderate, balanced approach</li>
<li><code>0.01</code> - Aggressive, use with caution</li>
<li>Lower is safer but slower</li>
</ul>
<p><strong>RATIO_TRAIN</strong>: Train/validation split</p>
<ul>
<li><code>0.9</code> = 90% training, 10% validation</li>
<li><code>0.8</code> = 80% training, 20% validation (better for smaller datasets)</li>
<li>Validation set helps detect overfitting</li>
</ul>
<h2 id="step-3-build-the-docker-environment">Step 3: Build the Docker Environment</h2>
<p>Clone or set up the project structure, then build the Docker image:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./build-image.sh
</span></span></code></pre></div><p>This creates a Docker image named <code>tesstrain-docker:1.0.0</code> with:</p>
<ul>
<li>Tesseract 5.x</li>
<li>tesstrain toolkit</li>
<li>Pre-trained language models</li>
<li>Python evaluation scripts</li>
<li>All dependencies pre-installed</li>
</ul>
<p>Build time: 5-10 minutes (one-time operation).</p>
<h2 id="step-4-run-training">Step 4: Run Training</h2>
<p>Place your prepared data in <code>tesseract-training-docker/WORKDIR/training_data/</code> and <code>tesseract-training-docker/WORKDIR/evaluation_data/</code>, then:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./run-auto.sh
</span></span></code></pre></div><h3 id="what-happens-during-training">What Happens During Training</h3>
<p>The script:</p>
<ol>
<li><strong>Validates configuration</strong> - Checks all required parameters</li>
<li><strong>Displays summary</strong> - Shows your settings before starting</li>
<li><strong>Mounts your data</strong> - Maps <code>WORKDIR</code> into the container</li>
<li><strong>Runs training pipeline</strong>:
<ul>
<li>Extracts features from images</li>
<li>Generates training files (.box, .lstmf files)</li>
<li>Splits data into train/validation sets</li>
<li>Runs LSTM training for specified iterations</li>
<li>Combines final model</li>
</ul>
</li>
<li><strong>Saves output</strong> - Writes <code>.traineddata</code> file to <code>WORKDIR/output/</code></li>
</ol>
<h3 id="monitoring-progress">Monitoring Progress</h3>
<p>Training output shows:</p>
<pre tabindex="0"><code>Iteration 100: Mean CER=3.45%, Word Accuracy=94.2%
Iteration 200: Mean CER=2.87%, Word Accuracy=95.8%
Iteration 300: Mean CER=2.34%, Word Accuracy=96.5%
...
</code></pre><p><strong>CER</strong> (Character Error Rate): Lower is better (0% = perfect)
<strong>WER</strong> (Word Error Rate): Lower is better (0% = perfect)</p>
<p>Watch for:</p>
<ul>
<li>‚úÖ Steady improvement in CER/WER</li>
<li>‚ö†Ô∏è Stagnation (might need more data or different parameters)</li>
<li>‚ùå Degradation (reduce learning rate or stop early)</li>
</ul>
<h2 id="step-5-evaluate-your-model">Step 5: Evaluate Your Model</h2>
<p>Evaluation compares your trained model against the baseline using held-out test data.</p>
<h3 id="evaluation-data">Evaluation Data</h3>
<p>For evaluation, make sure the folder tesseract-training-docker/WORKDIR/evaluation_data/ contains new images that were not used during training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>tesseract-training-docker/WORKDIR/evaluation_data/
</span></span><span style="display:flex;"><span>‚îú‚îÄ‚îÄ eval_sample_001.png
</span></span><span style="display:flex;"><span>‚îú‚îÄ‚îÄ eval_sample_001.gt.txt
</span></span><span style="display:flex;"><span>‚îú‚îÄ‚îÄ eval_sample_002.png
</span></span><span style="display:flex;"><span>‚îú‚îÄ‚îÄ eval_sample_002.gt.txt
</span></span><span style="display:flex;"><span>‚îî‚îÄ‚îÄ ...
</span></span></code></pre></div><p>Critical: Use <strong>different</strong> images than training. Using training data for evaluation gives falsely optimistic results.</p>
<h3 id="run-evaluation">Run Evaluation</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./run-evaluate.sh
</span></span></code></pre></div><p>This script:</p>
<ol>
<li>Validates trained model exists</li>
<li>Checks evaluation data is present</li>
<li>Runs OCR with baseline model</li>
<li>Runs OCR with your trained model</li>
<li>Calculates CER/WER metrics</li>
<li>Generates comparison report</li>
</ol>
<h3 id="understanding-results">Understanding Results</h3>
<p>Results are saved to <code>WORKDIR/output/evaluation_results/</code>:</p>
<p><strong>comparison_summary.txt</strong> - High-level metrics:</p>
<pre tabindex="0"><code>========================================
Tesseract Model Evaluation - Comparison
========================================
Evaluation Dataset:
  - Total images: 150
  - Successfully processed: 150
  - Total characters: 12,458
  - Total words: 2,341

----------------------------------------
BASELINE MODEL: srp_latn
----------------------------------------
  Character Error Rate (CER): 5.23%
  Word Error Rate (WER): 12.45%

----------------------------------------
TRAINED MODEL: my_custom_model_v1
----------------------------------------
  Character Error Rate (CER): 2.14%
  Word Error Rate (WER): 5.67%

----------------------------------------
IMPROVEMENT ANALYSIS
----------------------------------------
  CER improvement: 3.09% (‚úì Better)
  WER improvement: 6.78% (‚úì Better)
</code></pre><p><strong>What the metrics mean:</strong></p>
<ul>
<li>
<p><strong>CER (Character Error Rate)</strong>: Percentage of characters incorrectly recognized</p>
<ul>
<li>0% - perfect character recognition</li>
<li>2-3% - most characters are correct (acceptable for many uses)</li>
<li>7-8%+ - poor quality, needs improvement</li>
</ul>
</li>
<li>
<p><strong>WER (Word Error Rate)</strong>: Percentage of words with any errors</p>
<ul>
<li>0% - every word perfect</li>
<li>5-6% - majority of words correct (good for many uses)</li>
<li>10-12%+ - significant issues</li>
</ul>
</li>
</ul>
<h3 id="detailed-results">Detailed Results</h3>
<p><strong>baseline_detailed.txt</strong> and <strong>trained_model_detailed.txt</strong> show per-file metrics:</p>
<pre tabindex="0"><code>File: sample_042
  Ground Truth: –†–µ–ø—É–±–ª–∏–∫–∞ –°—Ä–±–∏—ò–∞
  OCR Result:   –†–µ–ø—É–±–ª–∏–∫–∞ –°—Ä–±–∏—ò–∞
  Characters: 15, Words: 2
  CER: 0.00%, WER: 0.00%
  Character Distance: 0, Word Distance: 0

File: sample_043
  Ground Truth: 21. –¥–µ—Ü–µ–º–±–∞—Ä 2025.
  OCR Result:   21. –¥–µ—Ü–µ–º–±–∞—Ä 2025.
  Characters: 18, Words: 3
  CER: 5.56%, WER: 33.33%
  Character Distance: 1, Word Distance: 1
</code></pre><p>Use these to identify:</p>
<ul>
<li>Problematic patterns (certain character combinations)</li>
<li>Specific font/style issues</li>
<li>Areas needing more training data</li>
</ul>
<h3 id="comparison-examples">Comparison Examples</h3>
<p>The summary file also includes side-by-side comparisons showing where each model made errors:</p>
<pre tabindex="0"><code>Example: form_header_001
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Ground Truth: Dr. Smith, M.D.
Baseline:     Dr. Smlth, M.D. (‚úó ERRORS)
New Model:    Dr. Smith, M.D. (‚úì PERFECT)
</code></pre><p>Errors are highlighted with <code>[‚úótext‚úó]</code> markers for easy identification.</p>
<h2 id="step-6-deployuse-fine-tuned-model">Step 6: Deploy/Use Fine Tuned Model</h2>
<p>After successful training and evaluation, your model is ready for deployment.</p>
<p>The simplest way to use it is to place the generated <code>.traineddata</code> file into the directory where Tesseract looks for language models (check your system to confirm the exact path).</p>
<ol>
<li>Copy the model to the tessdata directory</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo cp WORKDIR/output/my_custom_model_v1.traineddata /usr/share/tesseract-ocr/5/tessdata/
</span></span></code></pre></div><p>Note: The tessdata path may vary depending on your environment. Adjust it if necessary.</p>
<ol start="2">
<li>Run OCR with your model:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>tesseract input.png output -l my_custom_model_v1
</span></span></code></pre></div><p>That‚Äôs it. Tesseract will now use your fine-tuned model to process real-world data.</p>
<h2 id="tips--tricks">Tips &amp; Tricks</h2>
<h3 id="tips-for-better-results">Tips for Better Results</h3>
<ul>
<li>Start small, then iterate - Test with fewer iterations, evaluate metrics, then scale up.</li>
<li>Tune learning rate carefully - 0.0001‚Äì0.001 is usually safe for fine-tuning. Too high causes divergence, too low slows progress.</li>
<li>Watch for overfitting - If training accuracy improves but validation worsens, reduce iterations or add more diverse data.</li>
<li>Prioritize data quality - Clean, accurate ground truth beats large but noisy datasets.</li>
<li>Match real-world conditions - Train on data that reflects your production environment (fonts, scan quality, layout).</li>
</ul>
<h3 id="common-issues">Common Issues</h3>
<ul>
<li>Worse than baseline?
‚Üí Check ground truth, lower learning rate, verify base model choice.</li>
<li>Training too slow?
‚Üí Reduce iterations or start with a smaller dataset.</li>
<li>Good training, bad evaluation?
‚Üí Likely overfitting. Reduce iterations or increase validation split.</li>
<li>Errors on specific characters?
‚Üí Add more samples containing those characters.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Fine-tuning Tesseract OCR models doesn&rsquo;t have to be complicated. With Docker handling the environment complexity, you can focus on what matters:</p>
<ol>
<li><strong>Quality training data</strong> - Accurate ground truth is everything</li>
<li><strong>Proper configuration</strong> - Start conservative, iterate based on results</li>
<li><strong>Rigorous evaluation</strong> - Test on held-out data, not training samples</li>
<li><strong>Continuous improvement</strong> - Monitor production performance, retrain as needed</li>
</ol>
<p>The workflow we&rsquo;ve covered - prepare data, configure, train, evaluate - scales from quick experiments (100 samples, 100 iterations) to production models (1000+ samples, 1000+ iterations).</p>
<p>Start simple. Measure results. Iterate. Your custom model can achieve significant error reduction compared to generic models.</p>
<h2 id="next-steps">Next Steps</h2>
<ul>
<li>Try training on your first dataset (even 50 samples)</li>
<li>Experiment with different base models</li>
<li>Compare multiple parameter combinations</li>
<li>Set up automated evaluation in your CI/CD pipeline</li>
</ul>
<p>Happy training! üöÄ</p>

</div>
</div>
<div class="flex flex-row justify-around my-2">
  <h3 class="mb-1 mt-1 text-left mr-4">
    
    <a
      href="/blog/tesseract-fine-tuning-ocr/"
      title="Teaching Tesseract to Read Our Data"
    >
      <i class="nav-menu fas fa-chevron-circle-left"></i>
    </a>
    
  </h3>
  <h3 class="mb-1 mt-1 text-left ml-4">
    
    <i class="text-gray-300 dark:text-gray-600 fas fa-chevron-circle-right"></i>
    
  </h3>
</div>


    </main>
    
    <footer class="text-sm text-center border-t border-gray-500  py-6 ">
  <p class="markdownify">powered by <a href="https://gohugo.io/">hugo</a> &amp; deployed on <a href="https://pages.github.com/">GitHub Pages</a></p>
  <p >
    <i>
      <a href="https://nenadlazic.github.io/">
        ¬© 2025
      </a>
    </i>
    by
    <a href="https://github.com/nenadlazic">
      Nenad Laziƒá
    </a>
  </p>

  
</footer>

    
  </body>
</html>
